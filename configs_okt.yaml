##################################################
# exp_opts
##################################################
seed: 0
exp_name: 'okt' 
use_neptune: false
save_model: true
testing: false # only use a very small portion of the dataset for testing purposes
neptune_project: '' # change to your own "username/projectname"
neptune_api: ''  # change to your own neptune api
log_train_every_itr: 100
model_save_dir: "checkpoints"
##################################################
# data_opts
##################################################
data_path: "data"
data_for: 'okt' # choose from 'okt' or 'lstm' (student model), always 'okt' in this case
use_kc: false # if true, this need to be used together with combine_method='linear'
test_size: 0.2 # percentage of test dataset
max_len: 200 # maximum number of submission per student 
label_type: 'binary' # score division category, choose from 'binary', 'tenary' or 'raw'
first_ast_convertible: true # whether to use student first submission to each question
##################################################
# model_lstm_opts
##################################################
kt_model: 'lstm' # default="lstm", other options are 'akt' or 'dkvmn'
use_lstm: true # if false, this need to be used together with combine_method='exp_decay, kc_sim_decay, exp_kc_decay, no_decay'
lstm_inp_dim: 968 # Fixed at 968 as it's 768+200 (200 is the prompt embedding)
lstm_hid_dim: 768 # Fixed at 768 as it's GPT-2 token embedding size
use_classifier: false # if true, multi-task setting
classifier_hid_dim: 50 
lstm_init: 'rand' ## LSTM initialization, choose from 'rand' or 'zero'
pre_trained_lstm_path: null # use if pretrain LSTM model
pre_trained_classifier_path: null # use when use_classifier = true for multi-task setting
train_lstm: true 
lstm_lr: 0.00001
cls_lr: 0.001
##################################################
# model_gpt_opts
##################################################
okt_model: 'funcom' # pre-trained GPT model: choose from 'student', 'funcom' or 'gpt-2'
train_okt: true 
pad_token: 'pad' # padding token: choose from 'pad' or 'eos'
# combine knowledge state with input, choose from add, average, linear, weight, exp_decay, kc_sim_decay, exp_kc_decay, no_decay
combine_method: 'add'
combine_weight: 1 # combine weight of knowledge states
##################################################
# train_generator_opts
##################################################
epochs: 25
batch_size: 8
lr: 0.00001
lr_linear: 0.001
lr_weight: 0.001
use_scheduler: true # whether to use scheduler during optim
##################################################
# evaluation configs
##################################################
nsamples: 1
k: 1
p: 0
